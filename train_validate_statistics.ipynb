{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ad5d8a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6110d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Set workspace root\n",
    "WORKSPACE_ROOT = Path(os.getcwd())\n",
    "ADELAIDET_ROOT = WORKSPACE_ROOT / \"AdelaiDet\"\n",
    "MASKREFINE_ROOT = WORKSPACE_ROOT / \"MaskRefineNet\"\n",
    "DETECTRON2_ROOT = WORKSPACE_ROOT / \"detectron2\"\n",
    "\n",
    "print(f\"Workspace Root: {WORKSPACE_ROOT}\")\n",
    "print(f\"AdelaiDet Root: {ADELAIDET_ROOT}\")\n",
    "print(f\"MaskRefineNet Root: {MASKREFINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Dataset configuration\n",
    "    DATA_ROOT = \"YOUR_DATA_ROOT\"  # <-- Update this to your data root\n",
    "    DATASET = \"coco\"  # or \"BDD100K\"\n",
    "    SUBSET = \"5p\"  # Options: 1p, 2p, 5p, 10p, 20p, 30p, 50p\n",
    "    \n",
    "    # Training configuration\n",
    "    NUM_GPUS = 1  # Will be auto-detected if not specified\n",
    "    SEED = 1\n",
    "    \n",
    "    # Paths (auto-generated based on subset)\n",
    "    @property\n",
    "    def annotations_dir(self):\n",
    "        return Path(self.DATA_ROOT) / \"coco\" / \"annotations\"\n",
    "    \n",
    "    @property\n",
    "    def strong_json(self):\n",
    "        return f\"instances_train2017_{self.SUBSET}_s.json\"\n",
    "    \n",
    "    @property\n",
    "    def weak_json(self):\n",
    "        return f\"instances_train2017_{self.SUBSET}_w.json\"\n",
    "    \n",
    "    @property\n",
    "    def sw_refined_json(self):\n",
    "        return f\"instances_train2017_{self.SUBSET}_sw_refined.json\"\n",
    "\n",
    "config = Config()\n",
    "print(f\"Dataset: {config.DATASET}\")\n",
    "print(f\"Subset: {config.SUBSET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect number of GPUs\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        config.NUM_GPUS = torch.cuda.device_count()\n",
    "        print(f\"Detected {config.NUM_GPUS} GPU(s)\")\n",
    "        for i in range(config.NUM_GPUS):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"No GPU detected, using CPU (not recommended for training)\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04c3bb",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9327a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_annotations(json_path):\n",
    "    \"\"\"Load COCO format annotations\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def analyze_annotations(data, name=\"Dataset\"):\n",
    "    \"\"\"Analyze annotation statistics\"\"\"\n",
    "    stats = {\n",
    "        'name': name,\n",
    "        'num_images': len(data.get('images', [])),\n",
    "        'num_annotations': len(data.get('annotations', [])),\n",
    "        'num_categories': len(data.get('categories', [])),\n",
    "    }\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = defaultdict(int)\n",
    "    category_names = {cat['id']: cat['name'] for cat in data.get('categories', [])}\n",
    "    \n",
    "    for ann in data.get('annotations', []):\n",
    "        cat_id = ann['category_id']\n",
    "        category_counts[category_names.get(cat_id, f'cat_{cat_id}')] += 1\n",
    "    \n",
    "    stats['category_distribution'] = dict(category_counts)\n",
    "    \n",
    "    # Annotations per image\n",
    "    image_ann_counts = defaultdict(int)\n",
    "    for ann in data.get('annotations', []):\n",
    "        image_ann_counts[ann['image_id']] += 1\n",
    "    \n",
    "    ann_counts = list(image_ann_counts.values())\n",
    "    if ann_counts:\n",
    "        stats['avg_annotations_per_image'] = np.mean(ann_counts)\n",
    "        stats['max_annotations_per_image'] = max(ann_counts)\n",
    "        stats['min_annotations_per_image'] = min(ann_counts)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_stats(stats):\n",
    "    \"\"\"Print dataset statistics\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {stats['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Images: {stats['num_images']:,}\")\n",
    "    print(f\"  Annotations: {stats['num_annotations']:,}\")\n",
    "    print(f\"  Categories: {stats['num_categories']}\")\n",
    "    if 'avg_annotations_per_image' in stats:\n",
    "        print(f\"  Avg annotations/image: {stats['avg_annotations_per_image']:.2f}\")\n",
    "        print(f\"  Max annotations/image: {stats['max_annotations_per_image']}\")\n",
    "        print(f\"  Min annotations/image: {stats['min_annotations_per_image']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2851a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze available annotation files\n",
    "def analyze_all_subsets(annotations_dir):\n",
    "    \"\"\"Analyze all available annotation subsets\"\"\"\n",
    "    subsets = ['1p', '2p', '5p', '10p', '20p', '30p', '50p']\n",
    "    all_stats = []\n",
    "    \n",
    "    annotations_path = Path(annotations_dir)\n",
    "    \n",
    "    if not annotations_path.exists():\n",
    "        print(f\"Annotations directory not found: {annotations_path}\")\n",
    "        print(\"Please update config.DATA_ROOT to point to your data directory\")\n",
    "        return all_stats\n",
    "    \n",
    "    for subset in subsets:\n",
    "        strong_file = annotations_path / f\"instances_train2017_{subset}_s.json\"\n",
    "        weak_file = annotations_path / f\"instances_train2017_{subset}_w.json\"\n",
    "        \n",
    "        if strong_file.exists():\n",
    "            data = load_coco_annotations(strong_file)\n",
    "            stats = analyze_annotations(data, f\"{subset} Strong\")\n",
    "            all_stats.append(stats)\n",
    "            print_stats(stats)\n",
    "        \n",
    "        if weak_file.exists():\n",
    "            data = load_coco_annotations(weak_file)\n",
    "            stats = analyze_annotations(data, f\"{subset} Weak\")\n",
    "            all_stats.append(stats)\n",
    "            print_stats(stats)\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "# Run analysis\n",
    "all_stats = analyze_all_subsets(config.annotations_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e844f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_distribution(stats_list, top_n=20):\n",
    "    \"\"\"Plot category distribution for multiple datasets\"\"\"\n",
    "    if not stats_list:\n",
    "        print(\"No statistics available to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(stats_list), 1, figsize=(14, 4*len(stats_list)))\n",
    "    if len(stats_list) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, stats in zip(axes, stats_list):\n",
    "        cat_dist = stats.get('category_distribution', {})\n",
    "        if not cat_dist:\n",
    "            continue\n",
    "            \n",
    "        # Sort by count and take top N\n",
    "        sorted_cats = sorted(cat_dist.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        categories, counts = zip(*sorted_cats)\n",
    "        \n",
    "        ax.bar(range(len(categories)), counts, color='steelblue')\n",
    "        ax.set_xticks(range(len(categories)))\n",
    "        ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "        ax.set_title(f\"{stats['name']} - Top {top_n} Categories\")\n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions\n",
    "if all_stats:\n",
    "    plot_category_distribution(all_stats[:2])  # Plot first 2 subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f76516",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66961f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters for different subsets\n",
    "TRAINING_CONFIGS = {\n",
    "    '1p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (5000, 8000),\n",
    "        'train_iter': 10001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '2p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (15000, 20000),\n",
    "        'train_iter': 25001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '5p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (20000, 25000),\n",
    "        'train_iter': 30001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '10p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (35000, 40000),\n",
    "        'train_iter': 45001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '20p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (90000, 110000),\n",
    "        'train_iter': 120001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '30p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (120000, 150000),\n",
    "        'train_iter': 160001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "    '50p': {\n",
    "        'learning_rate': 0.05,\n",
    "        'decay_steps': (210000, 250000),\n",
    "        'train_iter': 270001,\n",
    "        'mrn_train_iters': 200000,\n",
    "        'mrn_warm_iters': 2000,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get config for current subset\n",
    "train_config = TRAINING_CONFIGS.get(config.SUBSET, TRAINING_CONFIGS['5p'])\n",
    "print(f\"Training Configuration for {config.SUBSET}:\")\n",
    "for key, value in train_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318bd22",
   "metadata": {},
   "source": [
    "## 4. Step 1: Train Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c880f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_teacher_training_command(config, train_config):\n",
    "    \"\"\"Generate command for training teacher network\"\"\"\n",
    "    exp_name = f\"SOLOv2_R101_coco{config.SUBSET}_teacher\"\n",
    "    trainsets = f\"('coco_2017_train_{config.SUBSET}_s',)\"\n",
    "    testsets = \"('coco_2017_val',)\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "export DETECTRON2_DATASETS={config.DATA_ROOT}\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus {config.NUM_GPUS} \\\\\n",
    "    SEED {config.SEED} \\\\\n",
    "    OUTPUT_DIR training_dir/{exp_name} \\\\\n",
    "    DATASETS.TRAIN {trainsets} \\\\\n",
    "    DATASETS.TEST {testsets} \\\\\n",
    "    SOLVER.STEPS {train_config['decay_steps']} \\\\\n",
    "    SOLVER.MAX_ITER {train_config['train_iter']} \\\\\n",
    "    SOLVER.BASE_LR {train_config['learning_rate']} \\\\\n",
    "    MODEL.SOLOV2.PROMPT point \\\\\n",
    "    MODEL.SOLOV2.EVAL_PSEUDO_LABEL True \\\\\n",
    "    TEST.EVAL_PERIOD 5000\n",
    "\"\"\"\n",
    "    return cmd, exp_name\n",
    "\n",
    "teacher_cmd, teacher_exp_name = generate_teacher_training_command(config, train_config)\n",
    "print(\"Teacher Network Training Command:\")\n",
    "print(teacher_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b33c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run teacher training (uncomment to execute)\n",
    "# Note: This will take a significant amount of time depending on your hardware\n",
    "\n",
    "RUN_TRAINING = False  # Set to True to run training\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    os.chdir(ADELAIDET_ROOT)\n",
    "    os.environ['DETECTRON2_DATASETS'] = config.DATA_ROOT\n",
    "    \n",
    "    # Run the training command\n",
    "    result = subprocess.run(\n",
    "        teacher_cmd.split('\\n')[-2].strip().replace('\\\\\\n', '').split(),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\", result.stderr)\n",
    "else:\n",
    "    print(\"Training is disabled. Set RUN_TRAINING = True to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81ee35",
   "metadata": {},
   "source": [
    "## 5. Step 2: Generate Pseudo Labels (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61959484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference_command(config, train_config, teacher_exp_name):\n",
    "    \"\"\"Generate command for generating pseudo labels\"\"\"\n",
    "    testsets = f\"('coco_2017_train_{config.SUBSET}_w',)\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "export DETECTRON2_DATASETS={config.DATA_ROOT}\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus {config.NUM_GPUS} \\\\\n",
    "    --eval-only \\\\\n",
    "    MODEL.WEIGHTS training_dir/{teacher_exp_name}/model_final.pth \\\\\n",
    "    OUTPUT_DIR inference_dir/{teacher_exp_name} \\\\\n",
    "    MODEL.SOLOV2.FPN_SCALE_RANGES \"((1,100000),(1,100000),(1,100000),(1,100000),(1,100000))\" \\\\\n",
    "    MODEL.SOLOV2.NMS_TYPE mask \\\\\n",
    "    MODEL.SOLOV2.PROMPT point \\\\\n",
    "    DATASETS.TEST {testsets}\n",
    "\"\"\"\n",
    "    return cmd\n",
    "\n",
    "inference_cmd = generate_inference_command(config, train_config, teacher_exp_name)\n",
    "print(\"Pseudo Label Generation Command:\")\n",
    "print(inference_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16150602",
   "metadata": {},
   "source": [
    "## 6. Step 3: Train MaskRefineNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mrn_training_command(config, train_config, teacher_exp_name):\n",
    "    \"\"\"Generate command for training MaskRefineNet\"\"\"\n",
    "    mrn_exp_name = f\"MRN_{config.SUBSET}\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {MASKREFINE_ROOT}\n",
    "\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node={config.NUM_GPUS} main.py \\\\\n",
    "    --data_root {config.DATA_ROOT} \\\\\n",
    "    --workspace results \\\\\n",
    "    --exp_name {mrn_exp_name} \\\\\n",
    "    --train_iters {train_config['mrn_train_iters']} \\\\\n",
    "    --warm_iters {train_config['mrn_warm_iters']} \\\\\n",
    "    --val_interval 5000 \\\\\n",
    "    --weak_pth ../AdelaiDet/inference_dir/{teacher_exp_name}_strong_1/inference/instances_predictions.pth \\\\\n",
    "               ../AdelaiDet/inference_dir/{teacher_exp_name}_strong_2/inference/instances_predictions.pth \\\\\n",
    "    --gt_json {config.strong_json} \\\\\n",
    "    --eval_pth ../AdelaiDet/inference_dir/{teacher_exp_name}/inference/instances_predictions.pth \\\\\n",
    "    --amp\n",
    "\"\"\"\n",
    "    return cmd, mrn_exp_name\n",
    "\n",
    "mrn_cmd, mrn_exp_name = generate_mrn_training_command(config, train_config, teacher_exp_name)\n",
    "print(\"MaskRefineNet Training Command:\")\n",
    "print(mrn_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803b8ad",
   "metadata": {},
   "source": [
    "## 7. Step 4: Merge Refined Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d482bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_command(config, teacher_exp_name, mrn_exp_name):\n",
    "    \"\"\"Generate command for merging strong and refined weak labels\"\"\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {MASKREFINE_ROOT}\n",
    "\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node={config.NUM_GPUS} merge_strong_and_refined_weak_labels.py \\\\\n",
    "    --data_root {config.DATA_ROOT} \\\\\n",
    "    --ckpt results/{mrn_exp_name}/ckpt/best_AP.pt \\\\\n",
    "    --dataset coco \\\\\n",
    "    --size 256 \\\\\n",
    "    --weak_pth ../AdelaiDet/inference_dir/{teacher_exp_name}/inference/instances_predictions.pth \\\\\n",
    "    --weak_json {config.DATA_ROOT}/coco/annotations/instances_train2017_{config.SUBSET}_w.json \\\\\n",
    "    --strong_json {config.DATA_ROOT}/coco/annotations/instances_train2017_{config.SUBSET}_s.json \\\\\n",
    "    --save_path {config.DATA_ROOT}/coco/annotations/instances_train2017_{config.SUBSET}_sw_refined.json\n",
    "\"\"\"\n",
    "    return cmd\n",
    "\n",
    "merge_cmd = generate_merge_command(config, teacher_exp_name, mrn_exp_name)\n",
    "print(\"Label Merging Command:\")\n",
    "print(merge_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb94867",
   "metadata": {},
   "source": [
    "## 8. Step 5: Train Student Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb678fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_student_training_command(config):\n",
    "    \"\"\"Generate command for training student network\"\"\"\n",
    "    exp_name = f\"SOLOv2_R101_coco{config.SUBSET}_sw_refined\"\n",
    "    trainsets = f\"('coco_2017_train_{config.SUBSET}_sw_refined',)\"\n",
    "    testsets = \"('coco_2017_val',)\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "export DETECTRON2_DATASETS={config.DATA_ROOT}\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/SOLOv2/R101_3x.yaml \\\\\n",
    "    --num-gpus {config.NUM_GPUS} \\\\\n",
    "    SEED {config.SEED} \\\\\n",
    "    OUTPUT_DIR training_dir/{exp_name} \\\\\n",
    "    DATASETS.TRAIN {trainsets} \\\\\n",
    "    DATASETS.TEST {testsets} \\\\\n",
    "    TEST.EVAL_PERIOD 5000\n",
    "\"\"\"\n",
    "    return cmd, exp_name\n",
    "\n",
    "student_cmd, student_exp_name = generate_student_training_command(config)\n",
    "print(\"Student Network Training Command:\")\n",
    "print(student_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04223f34",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_command(config, exp_name, model_path):\n",
    "    \"\"\"Generate command for model evaluation\"\"\"\n",
    "    testsets = \"('coco_2017_val',)\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "export DETECTRON2_DATASETS={config.DATA_ROOT}\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/SOLOv2/R101_3x.yaml \\\\\n",
    "    --num-gpus {config.NUM_GPUS} \\\\\n",
    "    --eval-only \\\\\n",
    "    MODEL.WEIGHTS {model_path} \\\\\n",
    "    OUTPUT_DIR evaluation_dir/{exp_name} \\\\\n",
    "    DATASETS.TEST {testsets}\n",
    "\"\"\"\n",
    "    return cmd\n",
    "\n",
    "# Evaluation command for the student network\n",
    "student_model_path = f\"training_dir/{student_exp_name}/model_final.pth\"\n",
    "eval_cmd = generate_evaluation_command(config, student_exp_name, student_model_path)\n",
    "print(\"Evaluation Command:\")\n",
    "print(eval_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6119c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected results comparison\n",
    "EXPECTED_RESULTS = {\n",
    "    '1p': 24.0,\n",
    "    '2p': 25.3,\n",
    "    '5p': 33.7,\n",
    "    '10p': 35.8,\n",
    "    '20p': 37.1,\n",
    "    '30p': 38.0,\n",
    "    '50p': 38.8,\n",
    "}\n",
    "\n",
    "def plot_expected_results():\n",
    "    \"\"\"Plot expected results from the paper\"\"\"\n",
    "    subsets = list(EXPECTED_RESULTS.keys())\n",
    "    mAPs = list(EXPECTED_RESULTS.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(subsets, mAPs, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mAP in zip(bars, mAPs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{mAP}%', ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.xlabel('Subset (% of fully labeled data)', fontsize=12)\n",
    "    plt.ylabel('COCO test-dev mAP (%)', fontsize=12)\n",
    "    plt.title('PointWSSIS Expected Results (from Paper)', fontsize=14)\n",
    "    plt.ylim(0, 45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_expected_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a25aeb",
   "metadata": {},
   "source": [
    "## 10. Training Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a240e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_training_log(log_path):\n",
    "    \"\"\"Parse training log to extract metrics\"\"\"\n",
    "    metrics = {\n",
    "        'iterations': [],\n",
    "        'total_loss': [],\n",
    "        'lr': [],\n",
    "        'time': [],\n",
    "        'segm_AP': [],\n",
    "    }\n",
    "    \n",
    "    if not Path(log_path).exists():\n",
    "        print(f\"Log file not found: {log_path}\")\n",
    "        return metrics\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Parse training iterations\n",
    "            if 'total_loss' in line:\n",
    "                # Extract iteration number\n",
    "                iter_match = re.search(r'iter: (\\d+)', line)\n",
    "                loss_match = re.search(r'total_loss: ([\\d.]+)', line)\n",
    "                lr_match = re.search(r'lr: ([\\d.e-]+)', line)\n",
    "                \n",
    "                if iter_match and loss_match:\n",
    "                    metrics['iterations'].append(int(iter_match.group(1)))\n",
    "                    metrics['total_loss'].append(float(loss_match.group(1)))\n",
    "                    if lr_match:\n",
    "                        metrics['lr'].append(float(lr_match.group(1)))\n",
    "            \n",
    "            # Parse evaluation metrics\n",
    "            if 'segm/AP' in line:\n",
    "                ap_match = re.search(r'segm/AP: ([\\d.]+)', line)\n",
    "                if ap_match:\n",
    "                    metrics['segm_AP'].append(float(ap_match.group(1)))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_training_metrics(metrics, title=\"Training Metrics\"):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    if not metrics['iterations']:\n",
    "        print(\"No training metrics to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss curve\n",
    "    axes[0, 0].plot(metrics['iterations'], metrics['total_loss'], 'b-', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Total Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    if metrics['lr']:\n",
    "        axes[0, 1].plot(metrics['iterations'], metrics['lr'], 'g-')\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Smoothed loss\n",
    "    if len(metrics['total_loss']) > 10:\n",
    "        window = min(100, len(metrics['total_loss']) // 10)\n",
    "        smoothed = np.convolve(metrics['total_loss'], np.ones(window)/window, mode='valid')\n",
    "        axes[1, 0].plot(metrics['iterations'][window-1:], smoothed, 'r-')\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Smoothed Loss')\n",
    "        axes[1, 0].set_title('Smoothed Training Loss')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AP over time\n",
    "    if metrics['segm_AP']:\n",
    "        axes[1, 1].plot(range(len(metrics['segm_AP'])), metrics['segm_AP'], 'mo-')\n",
    "        axes[1, 1].set_xlabel('Evaluation Index')\n",
    "        axes[1, 1].set_ylabel('Segmentation AP')\n",
    "        axes[1, 1].set_title('Validation AP')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Parse and plot training log\n",
    "# log_path = ADELAIDET_ROOT / \"training_dir\" / teacher_exp_name / \"log.txt\"\n",
    "# metrics = parse_training_log(log_path)\n",
    "# plot_training_metrics(metrics, f\"Teacher Network Training ({config.SUBSET})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b89cf4",
   "metadata": {},
   "source": [
    "## 11. Complete Training Pipeline Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a183260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complete_pipeline_script(config, train_config):\n",
    "    \"\"\"Generate a complete shell script for the entire training pipeline\"\"\"\n",
    "    \n",
    "    script = f\"\"\"#!/bin/bash\n",
    "# PointWSSIS Complete Training Pipeline\n",
    "# Subset: {config.SUBSET}\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "set -e  # Exit on error\n",
    "\n",
    "# Configuration\n",
    "ROOT=\"{config.DATA_ROOT}\"\n",
    "NGPUS={config.NUM_GPUS}\n",
    "SUBSET=\"{config.SUBSET}\"\n",
    "SEED={config.SEED}\n",
    "\n",
    "export DETECTRON2_DATASETS=${{ROOT}}\n",
    "\n",
    "# Step 1: Train Teacher Network\n",
    "echo \"=== Step 1: Training Teacher Network ===\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "EXP_NAME=\"SOLOv2_R101_coco${{SUBSET}}_teacher\"\n",
    "TRAINSETS=\"('coco_2017_train_${{SUBSET}}_s',)\"\n",
    "TESTSETS=\"('coco_2017_val',)\"\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus ${{NGPUS}} \\\\\n",
    "    SEED ${{SEED}} \\\\\n",
    "    OUTPUT_DIR training_dir/${{EXP_NAME}} \\\\\n",
    "    DATASETS.TRAIN ${{TRAINSETS}} \\\\\n",
    "    DATASETS.TEST ${{TESTSETS}} \\\\\n",
    "    SOLVER.STEPS {train_config['decay_steps']} \\\\\n",
    "    SOLVER.MAX_ITER {train_config['train_iter']} \\\\\n",
    "    SOLVER.BASE_LR {train_config['learning_rate']} \\\\\n",
    "    MODEL.SOLOV2.PROMPT point \\\\\n",
    "    MODEL.SOLOV2.EVAL_PSEUDO_LABEL True \\\\\n",
    "    TEST.EVAL_PERIOD 5000\n",
    "\n",
    "# Step 2: Generate pseudo labels for weak data\n",
    "echo \"=== Step 2: Generating Pseudo Labels ===\"\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus ${{NGPUS}} \\\\\n",
    "    --eval-only \\\\\n",
    "    MODEL.WEIGHTS training_dir/${{EXP_NAME}}/model_final.pth \\\\\n",
    "    OUTPUT_DIR inference_dir/${{EXP_NAME}} \\\\\n",
    "    MODEL.SOLOV2.FPN_SCALE_RANGES \"((1,100000),(1,100000),(1,100000),(1,100000),(1,100000))\" \\\\\n",
    "    MODEL.SOLOV2.NMS_TYPE mask \\\\\n",
    "    MODEL.SOLOV2.PROMPT point \\\\\n",
    "    DATASETS.TEST \"('coco_2017_train_${{SUBSET}}_w',)\"\n",
    "\n",
    "# Step 3: Generate predictions for strong data (for MRN training)\n",
    "echo \"=== Step 3: Generating Strong Data Predictions ===\"\n",
    "# Get intermediate checkpoints for diverse predictions\n",
    "CKPT_1=$(ls training_dir/${{EXP_NAME}}/model_*.pth | head -n 1)\n",
    "CKPT_2=$(ls training_dir/${{EXP_NAME}}/model_*.pth | tail -n 2 | head -n 1)\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus ${{NGPUS}} \\\\\n",
    "    --eval-only \\\\\n",
    "    MODEL.WEIGHTS ${{CKPT_1}} \\\\\n",
    "    OUTPUT_DIR inference_dir/${{EXP_NAME}}_strong_1 \\\\\n",
    "    MODEL.SOLOV2.NMS_TYPE mask \\\\\n",
    "    MODEL.SOLOV2.PROMPT point_with_size \\\\\n",
    "    DATASETS.TEST \"('coco_2017_train_${{SUBSET}}_s',)\"\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/PointWSSIS/R101_teacher.yaml \\\\\n",
    "    --num-gpus ${{NGPUS}} \\\\\n",
    "    --eval-only \\\\\n",
    "    MODEL.WEIGHTS ${{CKPT_2}} \\\\\n",
    "    OUTPUT_DIR inference_dir/${{EXP_NAME}}_strong_2 \\\\\n",
    "    MODEL.SOLOV2.NMS_TYPE mask \\\\\n",
    "    MODEL.SOLOV2.PROMPT point_with_size \\\\\n",
    "    DATASETS.TEST \"('coco_2017_train_${{SUBSET}}_s',)\"\n",
    "\n",
    "# Step 4: Train MaskRefineNet\n",
    "echo \"=== Step 4: Training MaskRefineNet ===\"\n",
    "cd {MASKREFINE_ROOT}\n",
    "\n",
    "MRN_EXP_NAME=\"MRN_${{SUBSET}}\"\n",
    "\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=${{NGPUS}} main.py \\\\\n",
    "    --data_root ${{ROOT}} \\\\\n",
    "    --workspace results \\\\\n",
    "    --exp_name ${{MRN_EXP_NAME}} \\\\\n",
    "    --train_iters {train_config['mrn_train_iters']} \\\\\n",
    "    --warm_iters {train_config['mrn_warm_iters']} \\\\\n",
    "    --val_interval 5000 \\\\\n",
    "    --weak_pth ../AdelaiDet/inference_dir/${{EXP_NAME}}_strong_1/inference/instances_predictions.pth \\\\\n",
    "               ../AdelaiDet/inference_dir/${{EXP_NAME}}_strong_2/inference/instances_predictions.pth \\\\\n",
    "    --gt_json instances_train2017_${{SUBSET}}_s.json \\\\\n",
    "    --eval_pth ../AdelaiDet/inference_dir/${{EXP_NAME}}/inference/instances_predictions.pth \\\\\n",
    "    --amp\n",
    "\n",
    "# Step 5: Merge labels\n",
    "echo \"=== Step 5: Merging Strong and Refined Weak Labels ===\"\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=${{NGPUS}} merge_strong_and_refined_weak_labels.py \\\\\n",
    "    --data_root ${{ROOT}} \\\\\n",
    "    --ckpt results/${{MRN_EXP_NAME}}/ckpt/best_AP.pt \\\\\n",
    "    --dataset coco \\\\\n",
    "    --size 256 \\\\\n",
    "    --weak_pth ../AdelaiDet/inference_dir/${{EXP_NAME}}/inference/instances_predictions.pth \\\\\n",
    "    --weak_json ${{ROOT}}/coco/annotations/instances_train2017_${{SUBSET}}_w.json \\\\\n",
    "    --strong_json ${{ROOT}}/coco/annotations/instances_train2017_${{SUBSET}}_s.json \\\\\n",
    "    --save_path ${{ROOT}}/coco/annotations/instances_train2017_${{SUBSET}}_sw_refined.json\n",
    "\n",
    "# Step 6: Train Student Network\n",
    "echo \"=== Step 6: Training Student Network ===\"\n",
    "cd {ADELAIDET_ROOT}\n",
    "\n",
    "STUDENT_EXP_NAME=\"SOLOv2_R101_coco${{SUBSET}}_sw_refined\"\n",
    "\n",
    "OMP_NUM_THREADS=1 python -W ignore tools/train_net.py \\\\\n",
    "    --config-file configs/SOLOv2/R101_3x.yaml \\\\\n",
    "    --num-gpus ${{NGPUS}} \\\\\n",
    "    SEED ${{SEED}} \\\\\n",
    "    OUTPUT_DIR training_dir/${{STUDENT_EXP_NAME}} \\\\\n",
    "    DATASETS.TRAIN \"('coco_2017_train_${{SUBSET}}_sw_refined',)\" \\\\\n",
    "    DATASETS.TEST \"('coco_2017_val',)\" \\\\\n",
    "    TEST.EVAL_PERIOD 5000\n",
    "\n",
    "echo \"=== Training Complete! ===\"\n",
    "echo \"Student model saved at: training_dir/${{STUDENT_EXP_NAME}}/model_final.pth\"\n",
    "\"\"\"\n",
    "    return script\n",
    "\n",
    "# Generate and display the complete script\n",
    "complete_script = generate_complete_pipeline_script(config, train_config)\n",
    "print(complete_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline script\n",
    "script_path = WORKSPACE_ROOT / f\"train_pipeline_{config.SUBSET}.sh\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(complete_script)\n",
    "print(f\"Pipeline script saved to: {script_path}\")\n",
    "print(f\"Run with: bash {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b5aae",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ed958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_dashboard():\n",
    "    \"\"\"Create a summary dashboard of the training pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PointWSSIS Training Pipeline Summary\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  - Dataset: {config.DATASET}\")\n",
    "    print(f\"  - Subset: {config.SUBSET} ({config.SUBSET.replace('p', '')}% fully labeled)\")\n",
    "    print(f\"  - Data Root: {config.DATA_ROOT}\")\n",
    "    print(f\"  - Number of GPUs: {config.NUM_GPUS}\")\n",
    "    \n",
    "    print(f\"\\nTraining Hyperparameters ({config.SUBSET}):\")\n",
    "    for key, value in train_config.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nPipeline Steps:\")\n",
    "    print(f\"  1. Train Teacher Network (SOLOv2)\")\n",
    "    print(f\"     - Config: configs/PointWSSIS/R101_teacher.yaml\")\n",
    "    print(f\"     - Iterations: {train_config['train_iter']}\")\n",
    "    print(f\"  2. Generate Pseudo Labels for Weak Data\")\n",
    "    print(f\"  3. Train MaskRefineNet\")\n",
    "    print(f\"     - Iterations: {train_config['mrn_train_iters']}\")\n",
    "    print(f\"  4. Merge Strong and Refined Weak Labels\")\n",
    "    print(f\"  5. Train Student Network\")\n",
    "    \n",
    "    print(f\"\\nExpected Results (from paper):\")\n",
    "    print(f\"  - COCO test-dev mAP: {EXPECTED_RESULTS.get(config.SUBSET, 'N/A')}%\")\n",
    "    \n",
    "    print(f\"\\nOutput Locations:\")\n",
    "    print(f\"  - Teacher model: AdelaiDet/training_dir/{teacher_exp_name}/\")\n",
    "    print(f\"  - MRN model: MaskRefineNet/results/{mrn_exp_name}/\")\n",
    "    print(f\"  - Student model: AdelaiDet/training_dir/{student_exp_name}/\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "create_summary_dashboard()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
